---
title: "MUSA508_Final_Project"
output: html_document
---
Points to hit in markdown: 
a. Motivate the analysis – “What is the use case; why would someone want to replicate your analysis and why would they use this approach?”
	b. Describe the data you used.
	c. Describe your exploratory analysis using maps and plots.
	d. What is the spatial or space/time process?
d. Describe your modeling approach and show how you arrived at your final model.
e. Validate your model with cross-validation and describe how your predictions are useful (accuracy vs. generalizability).
f. Provide additional maps and data visualizations to show that your model is useful.
g. Talk about how your analysis meets the use case you set out to address.
h. What could you do to make the analysis better?


# Setup
```{r setup_13, cache=TRUE, message=FALSE}
library(tidyverse)
library(sf)
library(lubridate)
library(tigris)
library(tidycensus)
library(viridis)
library(riem)
library(gridExtra)
library(knitr)
library(kableExtra)
library(RSocrata)
library(FNN)
library(caret)
library(plotROC)

plotTheme <- theme(
  plot.title =element_text(size=12),
  plot.subtitle = element_text(size=8),
  plot.caption = element_text(size = 6),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title.y = element_text(size = 10),
  # Set the entire chart region to blank
  panel.background=element_blank(),
  plot.background=element_blank(),
  #panel.border=element_rect(colour="#F0F0F0"),
  # Format the grid
  panel.grid.major=element_line(colour="#D0D0D0",size=.2),
  axis.ticks=element_blank())

mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2)
  )
}

palette5 <- c("#eff3ff","#bdd7e7","#6baed6","#3182bd","#08519c")
palette5b <- c("#f0f9e8","#bae4bc","#7bccc4","#43a2ca","#0868ac")
palette4 <- c("#D2FBD4","#92BCAB","#527D82","#123F5A")
palette2 <- c("#6baed6","#08519c")

```

```{r setup_2, cache=TRUE, message=FALSE}
nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
    output <-
      as.data.frame(nn) %>%
      rownames_to_column(var = "thisPoint") %>%
      gather(points, point_distance, V1:ncol(.)) %>%
      arrange(as.numeric(thisPoint)) %>%
      group_by(thisPoint) %>%
      dplyr::summarize(pointDistance = mean(point_distance)) %>%
      arrange(as.numeric(thisPoint)) %>% 
      dplyr::select(-thisPoint) %>%
      pull()
  
  return(output)  
}

qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}
q5 <- function(variable) {as.factor(ntile(variable, 5))}
```

```{r load_api_key, message = FALSE}
# Load API key
census_api_key("d8b938a81d19a2811d021f339295fbf6135f7d36", overwrite = TRUE)
```

```{r load_data}
# Read in the data with RSocrata package
inspections <- read.socrata(
  "https://data.cityofchicago.org/resource/4ijn-s7e5.json",
  app_token = "soPSENlY4ttB95Y2PJMQLdLQL",
  email     = "willi2eh@upenn.edu",
  password  = "Triscuit3!"
)

# Convert to sf object
# Filter for only restaurants that are in business
inspections <- st_as_sf(inspections, coords = c("location.latitude", "location.longitude"), 
                 crs = 4326, agr = "constant", na.fail=FALSE) %>%
   st_transform('ESRI:102271') %>%
  filter(results != "Out of Business") %>%
  filter(results != "Not Ready") %>%
  filter(facility_type == "Restaurant") 

# I had to just download this locally and read in: https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-City/ewy2-6yfk
ChicagoBoundary <- st_read("BoundariesCity.geojson")%>%
   st_transform('ESRI:102271') 

```

```{r feature_engineering, warning=FALSE}

# results_numeric
# Outcome variable recoded to numeric binary variable: Fail inspection = 1 & Pass inspection = 0
# If no entry, but violation -> fail
# If no entry, but no violations -> pass
# If pass with conditions -> pass
inspections <- inspections %>%
  mutate(results_numeric = ifelse(inspections$results == "Fail", 1, ifelse(((inspections$results == "No Entry")&(!is.na(inspections$violations))), 1, 0)))

# violations_count
# This code counts the number of times the character | is used (this separates the violations) and adds 1 (because there isn't one in front of the first)
library(stringr)
inspections <- inspections %>%
  mutate(violations_count = str_count(inspections$violations, coll("| "))) 
inspections <- inspections %>%
  mutate(violations_count = ifelse(!is.na(inspections$violations_count), inspections$violations_count + 1, 0))

# past_inspect & past_failed_inspect
# Create feature for how many previous inspections they have had & how many of those they have failed
# https://stackoverflow.com/questions/43957453/r-increasing-variable-based-on-previous-occurrences 
inspections <- inspections %>%
  arrange(dba_name, inspection_date) %>%
  group_by(dba_name) %>%
  mutate(total_inspect = 1:n()) %>%
  # Create a column showing fail_result, fail is 1, pass is 0
  mutate(fail_result = ifelse(results == "Fail", 1, 0)) %>%
  # Calculate the cumulative sum of fail_result
  mutate(failed_inspect = cumsum(fail_result)) %>%
  # Remove fail_result
  select(-fail_result) %>%
  # Sort the data frame by Date
  arrange(inspection_date)

inspections <- inspections %>%
  mutate(past_inspect = ifelse(total_inspect == 0, 0, total_inspect - 1)) %>%
  mutate(past_failed_inspect = ifelse(results == "Fail" & failed_inspect != 0, 
                                      failed_inspect - 1,
                                      failed_inspect)) %>%
  select(-total_inspect, -failed_inspect)

# no_of_days
# Create feature for days since last inspection
inspections <- inspections %>%
  arrange(dba_name, inspection_date) %>%
  group_by(dba_name) %>%
  mutate(no_of_days = round(c(NA, diff(inspection_date)), 0))

# days_operating
# Create feature for days the restaurant has been operating (days since first inspection)
inspections <- inspections %>%
  arrange(dba_name, inspection_date) %>%
  group_by(dba_name) %>% 
  mutate(days_operating = today() - min(ymd(inspection_date)))

# high_risk
# Create feature designating high risk vs everything else
inspections <- inspections %>%
  mutate(high_risk = ifelse(risk == "Risk 1 (High)", 1, 0))

```
```{r neighborhoods, cache = TRUE, message = FALSE, warning = FALSE, results = 'hide'}
# I had to just download this locally and read in: https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Neighborhoods/bbvz-uum9
chicagoNeighborhoods <- st_read("BoundariesNeighborhoods.geojson") %>%
   st_transform('ESRI:102271') 

# add neighborhoods to inspections data - this is an sf object
inspections <- st_join(inspections %>% st_transform(crs=4326),
        chicagoNeighborhoods %>%
          st_transform(crs=4326),
        join=st_intersects,
              left = TRUE) 

inspections <- inspections %>%
        select(-shape_area, -shape_len)

```

```{r inspections_2016}
# Subset for inspections in 2016 
# and then another df for 2017
# We will estimate a model using inspection data from 2016 to predict for 2017
# Doing these years because there was a change in violations in 7/2018 that may impact results: http://dev.cityofchicago.org/open%20data/data%20portal/2018/06/29/food-violations-changes.html
inspections16 <- inspections %>%
   dplyr::filter((inspection_date > as.POSIXct("2016-01-01")) & (inspection_date < as.POSIXct("2017-01-01")))

inspections17 <- inspections %>%
   dplyr::filter((inspection_date > as.POSIXct("2017-01-01")) & (inspection_date < as.POSIXct("2018-01-01")))

```

```{r use_nn_function}
# Pull in 311 Requests
# sanitation 
#sanitation <- read.socrata(
 # "https://data.cityofchicago.org/resource/me59-5fac.json",
  #app_token = "soPSENlY4ttB95Y2PJMQLdLQL",
  #email     = "willi2eh@upenn.edu",
  #password  = "Triscuit3!"
#)
# Convert to sf object & filter for 2016
#sanitation <- st_as_sf(sanitation, coords = c("latitude", "longitude"), 
 #                crs = 4326, agr = "constant", na.fail=FALSE) %>%
#  dplyr::filter((creation_date > as.POSIXct("2016-01-01")) & (creation_date < as.POSIXct("2017-01-01"))) %>%
 # st_transform(st_crs(inspections))

sanitation <-
  read.socrata("https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Sanitation-Code-Complaints-Hi/me59-5fac") %>%
    mutate(year = substr(creation_date,1,4)) %>% filter(year == "2016") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    mutate(Legend = "Sanitation") %>%
    st_transform(st_crs(inspections))

# rodents
rodents <-
  read.socrata("https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Rodent-Baiting-Historical/97t6-zrhs") %>%
    mutate(year = substr(creation_date,1,4)) %>% filter(year == "2016") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    mutate(Legend = "Rodents") %>%
    st_transform(st_crs(inspections))

# Create distance variables
st_c <- st_coordinates

# for some reason the column dba_name is causing an error on the nn_function - dropping it since aka_name is similar
inspections <- inspections %>%
  select(-dba_name) 

inspections <- na.omit(inspections)

inspections <-
  inspections %>%
    dplyr::mutate(
      sanitation_nn1 = nn_function(st_c(inspections), st_c(sanitation), 1),
      sanitation_nn2 = nn_function(st_c(inspections), st_c(sanitation), 2),
      sanitation_nn3 = nn_function(st_c(inspections), st_c(sanitation), 3),
      sanitation_nn4 = nn_function(st_c(inspections), st_c(sanitation), 4),
      sanitation_nn5 = nn_function(st_c(inspections), st_c(sanitation), 5))

inspections <-
  inspections %>%
    dplyr::mutate(
      rodents_nn1 = nn_function(st_c(inspections), st_c(rodents), 1),
      rodents_nn2 = nn_function(st_c(inspections), st_c(rodents), 2),
      rodents_nn3 = nn_function(st_c(inspections), st_c(rodents), 3),
      rodents_nn4 = nn_function(st_c(inspections), st_c(rodents), 4),
      rodents_nn5 = nn_function(st_c(inspections), st_c(rodents), 5))

```

```{r inspections_2016}
# Subset for inspections in 2016 
# and then another df for 2017
# We will estimate a model using inspection data from 2016 to predict for 2017
# Doing these years because there was a change in violations in 7/2018 that may impact results: http://dev.cityofchicago.org/open%20data/data%20portal/2018/06/29/food-violations-changes.html
inspections16 <- inspections %>%
   dplyr::filter((inspection_date > as.POSIXct("2016-01-01")) & (inspection_date < as.POSIXct("2017-01-01")))

inspections17 <- inspections %>%
   dplyr::filter((inspection_date > as.POSIXct("2017-01-01")) & (inspection_date < as.POSIXct("2018-01-01")))

```

```{r get_16_census, message=FALSE, warning=FALSE, cache=TRUE, results = 'hide'}
# Get 2016 census data
chicagoCensus <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E","B06011_001"), 
          year = 2016, state="IL", geometry=T, county=c("Cook"), output = "wide") %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         Median_Income = B06011_001E) %>%
  mutate(percentWhite = ((NumberWhites / TotalPop)*100),
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))

# combine inspection df and census df
inspections16 <- st_join(inspections16 %>% st_transform(crs=4326),
        chicagoCensus %>%
          st_transform(crs=4326),
        join=st_intersects,
              left = TRUE) 

```

```{r get_17_census, message=FALSE, warning=FALSE, cache=TRUE, results = 'hide'}
# Get 2017 census data
chicagoCensus <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E","B06011_001"), 
          year = 2017, state="IL", geometry=T, county=c("Cook"), output = "wide") %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         Median_Income = B06011_001E) %>%
  mutate(percentWhite = ((NumberWhites / TotalPop)*100),
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))

# combine inspection df and census df
inspections17 <- st_join(inspections17%>% st_transform(crs=4326),
        chicagoCensus %>%
          st_transform(crs=4326),
        join=st_intersects,
              left = TRUE) 

```

```{r import_weather_16, message = FALSE, warning = FALSE, cache = TRUE}
# Get 2016 weather data
weather.Panel <- 
  riem_measures(station = "ORD", date_start = "2016-01-01", date_end = "2016-12-31") %>%
  dplyr::select(valid, tmpf, p01i, sknt)%>%
  replace(is.na(.), 0) %>%
    mutate(inspection_date = as.Date(ymd_h(substr(valid,1,13)))) %>%
    mutate(week = week(inspection_date),
           dotw = wday(inspection_date, label=TRUE)) %>%
    group_by(inspection_date) %>%
    summarize(Temperature = max(tmpf),
              Precipitation = sum(p01i),
              Wind_Speed = max(sknt)) %>%
    mutate(Temperature = ifelse(Temperature == 0, 42, Temperature))

# Add weather data to inspections dataframe
inspections16 <- inspections16 %>%
 left_join(weather.Panel, by = "inspection_date") 

```

```{r import_weather_17, message = FALSE, warning = FALSE, cache = TRUE}
# Get 2017 weather data
weather.Panel <- 
  riem_measures(station = "ORD", date_start = "2017-01-01", date_end = "2017-12-31") %>%
  dplyr::select(valid, tmpf, p01i, sknt)%>%
  replace(is.na(.), 0) %>%
    mutate(inspection_date = as.Date(ymd_h(substr(valid,1,13)))) %>%
    mutate(week = week(inspection_date),
           dotw = wday(inspection_date, label=TRUE)) %>%
    group_by(inspection_date) %>%
    summarize(Temperature = max(tmpf),
              Precipitation = sum(p01i),
              Wind_Speed = max(sknt)) %>%
    mutate(Temperature = ifelse(Temperature == 0, 42, Temperature))

# Add weather data to inspections dataframe
inspections17 <- inspections17 %>%
 left_join(weather.Panel, by = "inspection_date") 

```

# Exploratory Data Analysis

First, we created some graphs of the model features and outcome variable. 

It can be seen that those restaurants in the "Risk 1 (High)" category are most likely to fail inspection.

```{r risk_graph}
# Create a graph that looks at the categorical feature "risk"

inspections16 %>%
  st_drop_geometry() %>%
  dplyr::mutate(Results = ifelse(results_numeric==1, "Fail", "Pass")) %>%
  dplyr::select(Results, risk) %>%
  gather(Variable, value, -Results) %>%
  count(Variable, value, Results) %>%
  ggplot(aes(value, n, fill = Results)) +   
    geom_bar(position = "dodge", stat="identity") +
    scale_fill_manual(values = palette2) +
    labs(x="Inspection Result", y="Count",
         title = "Risk Categories and Inspection Results") +
    plotTheme + theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
Restaurants that fail inspection seem to have fewer past inspections and past failed inspections, more violations, and more days since last inspection on average. Restaurants that fail inspection seem to have similar averages to the restaurants that pass inspection for temperature, days the restaurant has been operating, and median income and percent white of census tracts the restaurant is in. 

```{r continuous_graphs_1, message=FALSE, warning=FALSE}
# Check out differences in pass/fail for some of the continuous variables
inspections16 %>%
  st_drop_geometry() %>%
  dplyr::mutate(Results = ifelse(results_numeric==1, "Fail", "Pass")) %>%
  dplyr::select(Results, violations_count, past_inspect, past_failed_inspect, Temperature) %>%
  gather(Variable, value, -Results) %>%
    ggplot(aes(Results, value, fill=Results)) + 
      geom_bar(position = "dodge", stat = "summary", fun.y = "mean") + 
      facet_wrap(~Variable, scales = "free") +
      scale_fill_manual(values = palette2) +
      labs(x="Inspection Result", y="Value", 
           title = "Feature Associations with the Likelihood of Failed Inspection") +
      theme(legend.position = "none")
```

```{r continuous_graphs_2, message=FALSE, warning=FALSE}
# Check out differences in pass/fail for some more of the continuous variables
inspections16 %>%
  st_drop_geometry() %>%
  dplyr::mutate(Results = ifelse(results_numeric==1, "Fail", "Pass")) %>%
  dplyr::select(Results, no_of_days, days_operating, percentWhite, Median_Income) %>%
  gather(Variable, value, -Results) %>%
    ggplot(aes(Results, value, fill=Results)) + 
      geom_bar(position = "dodge", stat = "summary", fun.y = "mean") + 
      facet_wrap(~Variable, scales = "free") +
      scale_fill_manual(values = palette2) +
      labs(x="Inspection Result", y="Value", 
           title = "Feature Associations with the Likelihood of Failed Inspection") +
      theme(legend.position = "none")
```
Next we look at those restaurants that failed inspection across Chicago in 2016 (Figure ). It can be seen from Figure x and x that the majority of failed inspections occurred in the north east part of the city. The neighborhoods that had the most restaurants failing inspection were Lake View, Wicker Park/ West Town, and the Loop (Figure x). 

```{r failure_2016}

# Map of restaurants that failed inspection in 2016
inspections16_fail <- inspections16 %>% 
  filter(results_numeric == 1) %>%
  st_transform('ESRI:102271') 

ggplot() + 
  geom_sf(data=ChicagoBoundary) +
  geom_sf(data=chicagoNeighborhoods) +
  geom_sf(data=inspections16_fail, size=.3, color ="red") +
  labs(title = "Restaurants that Failed Inspection in 2016") +
  mapTheme()

```

```{r neighborhood_fails}
#Add variable for number of failed inspections by neighborhood
chicagoNeighborhoods$fail_count <- lengths(st_intersects(chicagoNeighborhoods, inspections16_fail))

ggplot(chicagoNeighborhoods) + 
  geom_sf(aes(fill = q5(fail_count))) +
  scale_fill_manual(values = palette5,labels = qBr(chicagoNeighborhoods, "fail_count"),name = "Inspection Failures\n(Quintile Breaks)") +
  labs(title = "Inspection Failures by Neighborhood") +
  mapTheme()
```

```{r 2016_inspections_by_neighborhood}
# bar plot of neighborhoods with the most failures (over 50)
inspections16_fail %>%
  st_drop_geometry() %>%
  dplyr::group_by(sec_neigh) %>%
  summarize(n = n()) %>%
  filter(n > 49) %>%
  filter(sec_neigh != "NA") %>%
  ggplot(aes(x=reorder(sec_neigh, n), y=n)) +
  geom_bar(stat = "identity", fill="#08519c") +
  coord_flip() +
  labs(x=" ", y="Count of Failed Inspections",
         title = "Count of Failed Inspections by Neighborhood") +
  plotTheme

```

Further, when we pull in census tract level data, we can see that the northeast part of the city where the most inspections occur is also home to Chicago's lower income census tracts (Figure x). 

```{r med_income_failure_2016}
# fixes projection issue
chicagoCensus <- chicagoCensus %>% 
   st_transform('ESRI:102271') 

# the census data has Cook County tracts that go beyond the city boundary - don't want to map those
selection <- 
  chicagoCensus[ChicagoBoundary,] 

ggplot(selection) + 
  geom_sf(aes(fill = q5(Median_Income))) +
  geom_sf(data=inspections16_fail, size=.2) +
  scale_fill_manual(values = rev(palette5b),labels = qBr(chicagoCensus, "Median_Income"),name = "Median_Income\n(Quintile Breaks)") +
  labs(title = "Inspection Failures and Median Income of Census Tracts ") +
  mapTheme()

```

Looking at the yearly weather trends (Figure x), we can see that wind speeds vary, but precipitation and temperature increase in spring and summer. 

```{r plot_weather, catche = TRUE}
grid.arrange(
  ggplot(weather.Panel, aes(inspection_date,Precipitation)) + geom_line() + 
  labs(title="Precipitation", x="Day", y="Precipitation") + plotTheme,
  ggplot(weather.Panel, aes(inspection_date,Wind_Speed)) + geom_line() + 
    labs(title="Wind Speed", x="Day", y="Wind Speed") + plotTheme,
  ggplot(weather.Panel, aes(inspection_date,Temperature)) + geom_line() + 
    labs(title="Temperature", x="Day", y="Temperature") + plotTheme,
  top="Weather Data - Chicago ORD - 2016")
```

The number of inspections done per day seems to vary randomly accross the year (Figure x). 

```{r inpsection_plot, cache = TRUE}
ggplot(dat_census %>%
         group_by(inspection_date) %>%
         tally())+
  geom_line(aes(x = inspection_date, y = n))+
  labs(title="Inspections per day in Chicago, 2016",
       x="Date", 
       y="Number of Inspections")+
  plotTheme
```

# Logistic Regression Model



```{r warning=FALSE}
# Train/test split
inspections16 <- na.omit(inspections16)
set.seed(33343)
trainIndex <- createDataPartition(y = paste(inspections16$sec_neigh, inspections16$results_numeric), p = .65, list = FALSE, times=1)
inspectionTrain <- inspections16[ trainIndex,]
inspectionTest  <- inspections16[-trainIndex,]
```

```{r reg1_model}
# Logistic regression model with all of the variables- Kitchen sink model- Reg1
reg1 <- glm(results_numeric ~ .,
                  data=inspectionTrain %>% st_drop_geometry() %>% dplyr::select(inspection_type, violations_count, past_inspect, past_failed_inspect, no_of_days, sanitation_nn1, sanitation_nn2, sanitation_nn3, sanitation_nn4, sanitation_nn5, rodents_nn1, rodents_nn2, rodents_nn3, rodents_nn4, rodents_nn5, sec_neigh, results_numeric, Temperature, Precipitation, Wind_Speed, days_operating, Median_Income, percentWhite, high_risk),
                  family="binomial" (link="logit"))
summary(reg1)
```

```{r reg2_model}
# Logistic regression model with selected variables - Reg2
reg2 <- glm(results_numeric ~ .,
                  data=inspectionTrain %>% st_drop_geometry() %>% dplyr::select(high_risk,  violations_count, past_failed_inspect, days_operating, Temperature, percentWhite, sec_neigh, results_numeric),
                  family="binomial" (link="logit"))
summary(reg2)

```

# Model Performance 

A confusion matrix is produced to show the goodness of fit of the model. A positive parameter is specified to let the function know that a value of 1 designates failing inspection. Our sensitivity (trust positive rate) is not great, but it is xxx which is an improvement from the kitchen sink model above. Alternatively, our specificity (true negative rate) is high at xxx.  

```{r model_metrics}
# Reg 2 model metrics
testProbs <- data.frame(Outcome = as.factor(inspectionTest$results_numeric),
                        Probs = predict(reg2, inspectionTest, type= "response"))

testProbs <- testProbs %>% mutate(predOutcome  = as.factor(ifelse(testProbs$Probs > 0.41 , 1, 0)))

caret::confusionMatrix(testProbs$predOutcome, testProbs$Outcome, 
                       positive = "1")
```

```{r}
cost_benefit_table <-
   testProbs %>%
      count(predOutcome, Outcome) %>%
      summarize(True_Negative = sum(n[predOutcome==0 & Outcome==0]),
                True_Positive = sum(n[predOutcome==1 & Outcome==1]),
                False_Negative = sum(n[predOutcome==0 & Outcome==1]),
                False_Positive = sum(n[predOutcome==1 & Outcome==0])) %>%
       gather(Variable, Count) %>%
    bind_cols(data.frame(Description = c(
              "We predicted pass and the restaurant passed inspection",
              "We predicted failure and the restaurant failed inspection",
              "We predicted pass and the restaurant failed inspection",
              "We predicted failure and the restaurant passed inspection")))
kable(cost_benefit_table) %>% 
  kable_styling(font_size = 12, full_width = F,
                bootstrap_options = c("striped", "hover", "condensed")) 
```

## Cross Validation

Next we cross validate our model to further assess goodness of fit and see if our model results hold across 100 held out test sets (instead of just 1). We can see the second model has a mean AUC is around xxx, which indicates a decent fit, but is inconsistent. Our model does not seem to generalize well with respect to Sensitivity, which is the rate it correctly predicts true positives (restaurants that fail inspection). Our model better generalized with respect to Specificity, which is the rate it correctly predicts true negatives (restaurants that pass inspection).  

```{r cv, message=FALSE, warning=FALSE}

ctrl <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)
cvFit.reg1 <- train(results_numeric ~ ., data = inspections16 %>% st_drop_geometry() %>%
                                   dplyr::select(inspection_type, violations_count, past_inspect, past_failed_inspect, no_of_days, sanitation_nn1, sanitation_nn2, sanitation_nn3, sanitation_nn4, sanitation_nn5, rodents_nn1, rodents_nn2, rodents_nn3, rodents_nn4, rodents_nn5, sec_neigh, results_numeric, Temperature, Precipitation, Wind_Speed, days_operating, Median_Income, percentWhite, high_risk) %>%
                      dplyr::mutate(results_numeric = ifelse(results_numeric==1,"fail","pass")),
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl)
cvFit.reg1
```

```{r cv, message=FALSE, warning=FALSE}

ctrl <- trainControl(method = "cv", number = 100, classProbs=TRUE, summaryFunction=twoClassSummary)
cvFit.reg2 <- train(results_numeric ~ ., data = inspections16 %>% st_drop_geometry() %>%
                                   dplyr::select(violations_count, high_risk, past_failed_inspect, days_operating, Temperature, percentWhite, sec_neigh, results_numeric) %>%
                      dplyr::mutate(results_numeric = ifelse(results_numeric==1,"fail","pass")),
                method="glm", family="binomial",
                metric="ROC", trControl = ctrl)
cvFit.reg2
```

```{r message=FALSE, warning=FALSE}
#Kitchen sink model metrics - Reg1
dplyr::select(cvFit.reg1$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit.reg1$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) + 
    geom_histogram(bins=35, fill = "#6baed6") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#08519c", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics Reg1",
         subtitle = "Across-fold mean reprented as dotted lines") +
    plotTheme
```

```{r}
# Better fitting model metrics - Reg2
dplyr::select(cvFit.reg2$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit.reg2$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) + 
    geom_histogram(bins=35, fill = "#6baed6") +
    facet_wrap(~metric) +
    geom_vline(aes(xintercept = mean), colour = "#08519c", linetype = 3, size = 1.5) +
    scale_x_continuous(limits = c(0, 1)) +
    labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics Reg2",
         subtitle = "Across-fold mean reprented as dotted lines") +
    plotTheme
```

## ROC Curve

The AUC (Area Under the Curve) for our model is xxx, which indicates a decent, useful fit. The graph supports this finding, with the curve being above the coin flip line but below the perfect fit line. 

```{r message=FALSE, warning=FALSE}
# ROC Curve
ggplot(testProbs, aes(d = as.numeric(testProbs$Outcome), m = Probs)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#08519c") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curve")
```

```{r message=FALSE, warning=FALSE}
pROC::auc(testProbs$Outcome, testProbs$Probs)
```

## Optimal Threshold

In this section, a function is created to iteratively loop through each threshold and calculate confusion metrics for each. The results are then examined by threshold to optimize. We find that the threshold of xxxx limits the false negatives (those we predict to pass but fail) and false positives (those we predict to fail but pass). We want to limit false negatives because incorrectly labeling a failed inspection as a pass means that customers using the app may be vulnerable to food borne illnesses at a restaurant they choose to go to. We want to limit false positives because it can be hurtful for businesses for their restaurants to be flagged when really they would pass inspection. We also want a threshold that keeps accuracy high. It looks like the optimal threshold for this model is xxx. 

```{r}
iterateThresholds <- function(data, observedClass, predictedProbs, group) {
#This function takes as its inputs, a data frame with an observed binomial class (1 or 0); a vector of predicted probabilities; and optionally a group indicator like race. It returns accuracy plus counts and rates of confusion matrix outcomes. It's a bit verbose because of the if (missing(group)). I don't know another way to make an optional parameter.
  observedClass <- enquo(observedClass)
  predictedProbs <- enquo(predictedProbs)
  group <- enquo(group)
  x = .01
  all_prediction <- data.frame()
  
  if (missing(group)) {
  
    while (x <= 1) {
    this_prediction <- data.frame()
    
    this_prediction <-
      data %>%
      mutate(predclass = ifelse(!!predictedProbs > x, 1,0)) %>%
      count(predclass, !!observedClass) %>%
      summarize(Count_TN = sum(n[predclass==0 & !!observedClass==0]),
                Count_TP = sum(n[predclass==1 & !!observedClass==1]),
                Count_FN = sum(n[predclass==0 & !!observedClass==1]),
                Count_FP = sum(n[predclass==1 & !!observedClass==0]),
                Rate_TP = Count_TP / (Count_TP + Count_FN),
                Rate_FP = Count_FP / (Count_FP + Count_TN),
                Rate_FN = Count_FN / (Count_FN + Count_TP),
                Rate_TN = Count_TN / (Count_TN + Count_FP),
                Accuracy = (Count_TP + Count_TN) / 
                           (Count_TP + Count_TN + Count_FN + Count_FP)) %>%
      mutate(Threshold = round(x,2))
    
    all_prediction <- rbind(all_prediction,this_prediction)
    x <- x + .01
  }
  return(all_prediction)
  }
  else if (!missing(group)) { 
   while (x <= 1) {
    this_prediction <- data.frame()
    
    this_prediction <-
      data %>%
      mutate(predclass = ifelse(!!predictedProbs > x, 1,0)) %>%
      group_by(!!group) %>%
      count(predclass, !!observedClass) %>%
      summarize(Count_TN = sum(n[predclass==0 & !!observedClass==0]),
                Count_TP = sum(n[predclass==1 & !!observedClass==1]),
                Count_FN = sum(n[predclass==0 & !!observedClass==1]),
                Count_FP = sum(n[predclass==1 & !!observedClass==0]),
                Rate_TP = Count_TP / (Count_TP + Count_FN),
                Rate_FP = Count_FP / (Count_FP + Count_TN),
                Rate_FN = Count_FN / (Count_FN + Count_TP),
                Rate_TN = Count_TN / (Count_TN + Count_FP),
                Accuracy = (Count_TP + Count_TN) / 
                           (Count_TP + Count_TN + Count_FN + Count_FP)) %>%
      mutate(Threshold = round(x,2))
    
    all_prediction <- rbind(all_prediction,this_prediction)
    x <- x + .01
  }
  return(all_prediction)
  }
}
```

```{r}
whichThreshold <- 
  iterateThresholds(
     data=testProbs, observedClass = Outcome, predictedProbs = Probs)
whichThreshold <- as.data.frame(whichThreshold)
```

## Model Performance on Next Year's Data (2017)

```{r model_metrics_17}
# In the 2017 inspection data, the KENWOOD,OAKLAND neighborhood has two entries that are instead "OAKLAND,KENWOOD", so I fix those
inspections17$sec_neigh[inspections17$sec_neigh == "OAKLAND,KENWOOD"] <- "KENWOOD,OAKLAND"

# Reg 2 model metrics
testProbs <- data.frame(Outcome = as.factor(inspections17$results_numeric),
                        Probs = predict(reg2, inspections17, type= "response"))

testProbs <- testProbs %>% mutate(predOutcome  = as.factor(ifelse(testProbs$Probs > 0.41 , 1, 0)))

caret::confusionMatrix(testProbs$predOutcome, testProbs$Outcome, 
                       positive = "1")
```

## Model Performance on Next Year's Data (2018)

show model getting worse??














OLD CODE: 

```{r}
reg.vars <- c("rodents_nn5", "sanitation_nn5", "violations_count", "risk")

inspections16 <- inspections16 %>%
  mutate(cvID = sample(round(nrow(inspections16) / 120), size=nrow(inspections16), replace = TRUE))

crossValidate <- function(dataset, id, dependentVariable, indVariables) {

allPredictions <- data.frame()
cvID_list <- unique(dataset[[id]])

for (i in cvID_list) {

  thisFold <- i
  cat("This hold out fold is", thisFold, "\n")

  fold.train <- filter(dataset, dataset[[id]] != thisFold) %>% as.data.frame() %>% 
                dplyr::select(id, geometry, indVariables, dependentVariable)
  fold.test  <- filter(dataset, dataset[[id]] == thisFold) %>% as.data.frame() %>% 
                dplyr::select(id, geometry, indVariables, dependentVariable)
  
  regression <-
    glm(results_numeric ~ ., family = "binomial" (link="logit"), 
      data = fold.train %>% 
      dplyr::select(-geometry, -id))
  
  thisPrediction <- 
    mutate(fold.test, Prediction = predict(regression, fold.test, type = "response"))
    
  allPredictions <-
    rbind(allPredictions, thisPrediction)
    
  }
  return(st_sf(allPredictions))
}
```

```{r message=FALSE, warning=FALSE}
reg.cv <- crossValidate(
  dataset = inspections16,
  id = "cvID",
  dependentVariable = "results_numeric",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = cvID, inspections16, Prediction, geometry)

```


# inspectionCount
# Create feature for # of previous inspections
#inspections <- inspections %>%
 # arrange(inspection_date) %>%
  # group_by(dba_name) %>%
   #mutate(inspectionCount = row_number() - 1L) %>%
   #ungroup()
   
```{r get_census, message=FALSE, warning=FALSE, cache=TRUE, results = 'hide'}
## don't think we need this but will leave here for now
chicagoCensus <- 
  get_acs(geography = "tract", 
          variables = c("B01003_001", "B19013_001", 
                        "B02001_002", "B08013_001",
                        "B08012_001", "B08301_001", 
                        "B08301_010", "B01002_001"), 
          year = 2017, 
          state = "IL", 
          geometry = TRUE, 
          county=c("Cook"),
          output = "wide") %>%
  rename(Total_Pop =  B01003_001E,
         Med_Inc = B19013_001E,
         Med_Age = B01002_001E,
         White_Pop = B02001_002E,
         Travel_Time = B08013_001E,
         Num_Commuters = B08012_001E,
         Means_of_Transport = B08301_001E,
         Total_Public_Trans = B08301_010E) %>%
  select(Total_Pop, Med_Inc, White_Pop, Travel_Time,
         Means_of_Transport, Total_Public_Trans,
         Med_Age,
         GEOID, geometry) %>%
  mutate(Percent_White = White_Pop / Total_Pop,
         Mean_Commute_Time = Travel_Time / Total_Public_Trans,
         Percent_Taking_Public_Trans = Total_Public_Trans / Means_of_Transport)

```



```{r extract_geometries, cache = TRUE}
chicagoTracts <- 
  chicagoCensus %>%
  as.data.frame() %>%
  distinct(GEOID, .keep_all = TRUE) %>%
  select(GEOID, geometry) %>% 
  st_sf 

inspections16 <- inspections16 %>% st_transform(4326) 

```

```{r add_census_tracts, cache = TRUE, message = FALSE, warning = FALSE}
# this creates a new data frame with inspections and census data - not an sf object
dat_census <- st_join(inspections16 %>% 
          filter(is.na(location.latitude) == FALSE &
                   is.na(location.longitude) == FALSE) %>%
          st_as_sf(., coords = c("location.longitude", "location.latitude"), crs = 4326),
        chicagoTracts %>%
          st_transform(crs=4326),
        join=st_intersects,
              left = TRUE) %>%
  rename(Origin.Tract = GEOID) %>%
  mutate(location.longitude = unlist(map(geometry, 1)),
         location.latitude = unlist(map(geometry, 2)))%>%
  as.data.frame() %>%
  select(-geometry)
 
```
